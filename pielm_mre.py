"""
PIELM-MRE: Physics-Informed Extreme Learning Machine for MRE Inverse Problem
==============================================================================

Implements Iterative PIELM with Curriculum Learning for solving the coupled
MRE inverse problem: finding stiffness Î¼(x) from measured displacement u(x).

Architecture: Dual ELM Networks
- Network A (Displacement): X â†’ u(x,y,z) [complex-valued]
- Network B (Modulus): X â†’ Î¼(x,y,z) [complex-valued]

Training Strategy: Iterative Alternating Optimization
1. Fix Î¼, solve for u (data matching + physics)
2. Fix u, solve for Î¼ (physics + smoothness)
3. Repeat until convergence

Curriculum Learning:
- Stage 1: High regularization (near-homogeneous)
- Stage 2: Medium regularization
- Stage 3: Full heterogeneity

Author: Yeshwanth Kesav
Date: November 2025
"""

import numpy as np
from typing import Dict, Tuple, Callable, Optional
import matplotlib.pyplot as plt
from scipy.stats.qmc import Halton


class PIELMFeatures:
    """
    Random feature generator for ELM with automatic differentiation support.
    
    Uses tanh activation: Ï†(z) = tanh(WxÂ·x + WyÂ·y + WzÂ·z + b)
    
    Provides analytical derivatives:
    - Ï†_x, Ï†_y, Ï†_z (first-order)
    - Ï†_xx, Ï†_yy, Ï†_zz (second-order diagonal)
    - Ï†_xy, Ï†_xz, Ï†_yz (second-order cross)
    """
    
    def __init__(self, n_neurons: int, dim: int = 3, seed: int = 42):
        """
        Initialize random feature layer.
        
        Args:
            n_neurons: Number of hidden neurons
            dim: Input dimension (3 for MRE: x,y,z)
            seed: Random seed for reproducibility
        """
        rng = np.random.default_rng(seed)
        
        # Random weights for each spatial dimension
        self.Wx = rng.normal(size=n_neurons) if dim >= 1 else np.zeros(n_neurons)
        self.Wy = rng.normal(size=n_neurons) if dim >= 2 else np.zeros(n_neurons)
        self.Wz = rng.normal(size=n_neurons) if dim >= 3 else np.zeros(n_neurons)
        self.b = rng.normal(size=n_neurons)
        
        self.n_neurons = n_neurons
        self.dim = dim
        
        print(f"âœ… PIELM Features initialized:")
        print(f"   Neurons: {n_neurons}")
        print(f"   Dimension: {dim}")
        print(f"   Activation: tanh")
    
    def _z(self, X: np.ndarray) -> np.ndarray:
        """
        Compute pre-activation: z = WxÂ·x + WyÂ·y + WzÂ·z + b
        
        Args:
            X: Input coordinates (N, 3) for [x, y, z]
        
        Returns:
            Pre-activation (N, n_neurons)
        """
        z = np.outer(X[:, 0], self.Wx) + self.b
        if self.dim >= 2:
            z += np.outer(X[:, 1], self.Wy)
        if self.dim >= 3:
            z += np.outer(X[:, 2], self.Wz)
        return z
    
    # ========== Activation and derivatives ==========
    def phi(self, X: np.ndarray) -> np.ndarray:
        """Ï†(z) = tanh(z)"""
        return np.tanh(self._z(X))
    
    def phi_x(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Ï†/âˆ‚x = sechÂ²(z) Â· Wx"""
        z = self._z(X)
        sech2 = (1 / np.cosh(z))**2
        return sech2 * self.Wx
    
    def phi_y(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Ï†/âˆ‚y = sechÂ²(z) Â· Wy"""
        if self.dim < 2:
            return np.zeros((X.shape[0], self.n_neurons))
        z = self._z(X)
        sech2 = (1 / np.cosh(z))**2
        return sech2 * self.Wy
    
    def phi_z(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Ï†/âˆ‚z = sechÂ²(z) Â· Wz"""
        if self.dim < 3:
            return np.zeros((X.shape[0], self.n_neurons))
        z = self._z(X)
        sech2 = (1 / np.cosh(z))**2
        return sech2 * self.Wz
    
    def phi_xx(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Â²Ï†/âˆ‚xÂ² = -2Â·tanh(z)Â·sechÂ²(z)Â·WxÂ²"""
        z = self._z(X)
        return -2 * np.tanh(z) * (1 / np.cosh(z))**2 * (self.Wx**2)
    
    def phi_yy(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Â²Ï†/âˆ‚yÂ² = -2Â·tanh(z)Â·sechÂ²(z)Â·WyÂ²"""
        if self.dim < 2:
            return np.zeros((X.shape[0], self.n_neurons))
        z = self._z(X)
        return -2 * np.tanh(z) * (1 / np.cosh(z))**2 * (self.Wy**2)
    
    def phi_zz(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Â²Ï†/âˆ‚zÂ² = -2Â·tanh(z)Â·sechÂ²(z)Â·WzÂ²"""
        if self.dim < 3:
            return np.zeros((X.shape[0], self.n_neurons))
        z = self._z(X)
        return -2 * np.tanh(z) * (1 / np.cosh(z))**2 * (self.Wz**2)
    
    def phi_xy(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Â²Ï†/âˆ‚xâˆ‚y = -2Â·tanh(z)Â·sechÂ²(z)Â·WxÂ·Wy"""
        if self.dim < 2:
            return np.zeros((X.shape[0], self.n_neurons))
        z = self._z(X)
        return -2 * np.tanh(z) * (1 / np.cosh(z))**2 * (self.Wx * self.Wy)
    
    def phi_xz(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Â²Ï†/âˆ‚xâˆ‚z = -2Â·tanh(z)Â·sechÂ²(z)Â·WxÂ·Wz"""
        if self.dim < 3:
            return np.zeros((X.shape[0], self.n_neurons))
        z = self._z(X)
        return -2 * np.tanh(z) * (1 / np.cosh(z))**2 * (self.Wx * self.Wz)
    
    def phi_yz(self, X: np.ndarray) -> np.ndarray:
        """âˆ‚Â²Ï†/âˆ‚yâˆ‚z = -2Â·tanh(z)Â·sechÂ²(z)Â·WyÂ·Wz"""
        if self.dim < 3:
            return np.zeros((X.shape[0], self.n_neurons))
        z = self._z(X)
        return -2 * np.tanh(z) * (1 / np.cosh(z))**2 * (self.Wy * self.Wz)
    
    def laplacian(self, X: np.ndarray) -> np.ndarray:
        """
        Compute Laplacian: âˆ‡Â²Ï† = âˆ‚Â²Ï†/âˆ‚xÂ² + âˆ‚Â²Ï†/âˆ‚yÂ² + âˆ‚Â²Ï†/âˆ‚zÂ²
        
        Returns:
            (N, n_neurons) array
        """
        return self.phi_xx(X) + self.phi_yy(X) + self.phi_zz(X)


class PIELMNetwork:
    """
    Single ELM network for either displacement or modulus.
    
    Network structure:
        Input (x,y,z) â†’ Hidden Layer (random tanh features) â†’ Output
    
    For complex-valued outputs (MRE), we use:
        - Output_real from weights_real
        - Output_imag from weights_imag
    """
    
    def __init__(self, features: PIELMFeatures, is_complex: bool = True):
        """
        Initialize ELM network.
        
        Args:
            features: Random feature generator
            is_complex: Whether output is complex-valued
        """
        self.features = features
        self.is_complex = is_complex
        
        # Output weights (solved via least squares)
        self.weights_real = None
        self.weights_imag = None if is_complex else None
        
        self.is_trained = False
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predict output for given inputs.
        
        Args:
            X: Input coordinates (N, 3)
        
        Returns:
            Predictions (N,) - complex if is_complex=True
        """
        if not self.is_trained:
            raise RuntimeError("Network not trained yet!")
        
        Phi = self.features.phi(X)  # (N, n_neurons)
        
        if self.is_complex:
            u_real = Phi @ self.weights_real
            u_imag = Phi @ self.weights_imag
            return u_real + 1j * u_imag
        else:
            return Phi @ self.weights_real
    
    def solve_weights(
        self,
        H: np.ndarray,
        b: np.ndarray,
        ridge: float = 1e-8
    ):
        """
        Solve for output weights using ridge regression.
        
        Args:
            H: Feature matrix (M, n_neurons)
            b: Target vector (M,) or (M, 2) for complex
            ridge: Regularization parameter
        """
        # Solve: (H^T H + Î»I)w = H^T b
        HtH = H.T @ H
        HtH += ridge * np.eye(H.shape[1])
        Htb_real = H.T @ b.real
        
        self.weights_real = np.linalg.solve(HtH, Htb_real)
        
        if self.is_complex:
            Htb_imag = H.T @ b.imag
            self.weights_imag = np.linalg.solve(HtH, Htb_imag)
        
        self.is_trained = True


class IterativePIELMMRE:
    """
    Iterative PIELM solver for MRE inverse problem.
    
    Solves for both displacement u(x) and modulus Î¼(x) by alternating:
    1. Fix Î¼, optimize u (data matching + physics)
    2. Fix u, optimize Î¼ (physics + smoothness)
    
    Implements curriculum learning to progressively reduce regularization.
    """
    
    def __init__(
        self,
        n_neurons: int = 1000,
        frequency: float = 60.0,
        density: float = 1000.0,
        seed: int = 42
    ):
        """
        Initialize iterative PIELM-MRE solver.
        
        Args:
            n_neurons: Number of hidden neurons per network
            frequency: MRE excitation frequency (Hz)
            density: Tissue density (kg/mÂ³)
            seed: Random seed
        """
        # Create feature generators for both networks
        self.features_u = PIELMFeatures(n_neurons, dim=3, seed=seed)
        self.features_mu = PIELMFeatures(n_neurons, dim=3, seed=seed+1)
        
        # Create networks
        self.u_network = PIELMNetwork(self.features_u, is_complex=True)
        self.mu_network = PIELMNetwork(self.features_mu, is_complex=True)
        
        # Physics parameters
        self.frequency = frequency
        self.omega = 2 * np.pi * frequency
        self.density = density
        self.rho_omega_sq = density * self.omega**2
        
        # Training history
        self.history = {
            'iteration': [],
            'loss_total': [],
            'loss_data': [],
            'loss_physics_u': [],
            'loss_physics_mu': [],
            'loss_reg': []
        }
        
        print(f"\n{'='*60}")
        print("Iterative PIELM-MRE Solver Initialized")
        print(f"{'='*60}")
        print(f"ğŸ“Š Network Configuration:")
        print(f"   Hidden neurons (each net): {n_neurons}")
        print(f"   Displacement network: Complex-valued ELM")
        print(f"   Modulus network: Complex-valued ELM")
        print(f"\nâš™ï¸  Physics Parameters:")
        print(f"   Frequency: {frequency} Hz")
        print(f"   Angular frequency (Ï‰): {self.omega:.2f} rad/s")
        print(f"   Density (Ï): {density} kg/mÂ³")
        print(f"   ÏÏ‰Â²: {self.rho_omega_sq:.2e} kg/(mÂ·sÂ²)")
        print(f"{'='*60}\n")
    
    def assemble_displacement_system(
        self,
        X_data: np.ndarray,
        u_measured: np.ndarray,
        X_colloc: np.ndarray,
        mu_current: np.ndarray,
        lambda_data: float = 1.0,
        lambda_physics: float = 0.5
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Assemble weighted least squares system for displacement network.
        
        Given fixed Î¼, solve for u that minimizes:
            L = Î»_data ||u - u_measured||Â² + Î»_physics ||PDE_residual||Â²
        
        PDE: âˆ‡Â·[Î¼âˆ‡u] + ÏÏ‰Â²u = 0
        Expanded: Î¼âˆ‡Â²u + âˆ‡Î¼Â·âˆ‡u + ÏÏ‰Â²u = 0
        
        Args:
            X_data: Data points (N_data, 3)
            u_measured: Measured displacement (N_data,) complex
            X_colloc: Collocation points for physics (N_colloc, 3)
            mu_current: Current modulus estimate (N_colloc,) complex
            lambda_data: Weight for data loss
            lambda_physics: Weight for physics loss
        
        Returns:
            H: Feature matrix (M, n_neurons)
            b: Target vector (M,) complex
        """
        feat = self.features_u
        
        # ========== Data Loss Rows ==========
        Phi_data = feat.phi(X_data)  # (N_data, n_neurons)
        H_data = np.sqrt(lambda_data) * Phi_data
        b_data = np.sqrt(lambda_data) * u_measured
        
        # ========== Physics Loss Rows ==========
        # For each collocation point, compute PDE residual row
        N_colloc = X_colloc.shape[0]
        
        # Get feature derivatives at collocation points
        Phi_colloc = feat.phi(X_colloc)  # (N_colloc, n_neurons)
        Phi_x = feat.phi_x(X_colloc)
        Phi_y = feat.phi_y(X_colloc)
        Phi_z = feat.phi_z(X_colloc)
        Laplacian_phi = feat.laplacian(X_colloc)  # âˆ‡Â²Ï†
        
        # Compute âˆ‡Î¼ at collocation points (need to evaluate Î¼_network derivatives)
        # For now, approximate âˆ‡Î¼ using finite differences or assume we have it
        # TODO: This requires Î¼_network to provide derivatives
        # For simplicity, let's assume âˆ‡Î¼ â‰ˆ 0 for first iteration (homogeneous approximation)
        
        # Heterogeneous Helmholtz: Î¼âˆ‡Â²u + âˆ‡Î¼Â·âˆ‡u + ÏÏ‰Â²u = 0
        # If we neglect âˆ‡Î¼Â·âˆ‡u term initially:
        # Î¼âˆ‡Â²u + ÏÏ‰Â²u = 0
        
        # Rows for physics: [Î¼Â·âˆ‡Â²Ï† + ÏÏ‰Â²Â·Ï†] @ weights = 0
        H_physics = np.sqrt(lambda_physics) * (
            mu_current[:, None] * Laplacian_phi + self.rho_omega_sq * Phi_colloc
        )
        b_physics = np.zeros(N_colloc, dtype=complex)
        
        # ========== Stack ==========
        H = np.vstack([H_data, H_physics])
        b = np.concatenate([b_data, b_physics])
        
        return H, b
    
    def assemble_modulus_system(
        self,
        X_colloc: np.ndarray,
        u_current: np.ndarray,
        grad_u_current: np.ndarray,
        laplacian_u_current: np.ndarray,
        lambda_physics: float = 1.0,
        lambda_reg: float = 0.1
    ) -> Tuple[np.ndarray, np.ndarray]:
        """
        Assemble weighted least squares system for modulus network.
        
        Given fixed u, solve for Î¼ that minimizes:
            L = Î»_physics ||PDE_residual||Â² + Î»_reg ||âˆ‡Î¼||Â²
        
        From PDE: Î¼âˆ‡Â²u + âˆ‡Î¼Â·âˆ‡u + ÏÏ‰Â²u = 0
        Rearranged: Î¼ = -(âˆ‡Î¼Â·âˆ‡u + ÏÏ‰Â²u) / âˆ‡Â²u
        
        But this is implicit. Instead, we solve:
            Î¼Â·âˆ‡Â²u = -(âˆ‡Î¼Â·âˆ‡u + ÏÏ‰Â²u)
        
        Simplified (neglecting âˆ‡Î¼Â·âˆ‡u for stability):
            Î¼Â·âˆ‡Â²u â‰ˆ -ÏÏ‰Â²u
        
        Args:
            X_colloc: Collocation points (N_colloc, 3)
            u_current: Current displacement (N_colloc,) complex
            grad_u_current: Current âˆ‡u (N_colloc, 3) complex
            laplacian_u_current: Current âˆ‡Â²u (N_colloc,) complex
            lambda_physics: Weight for physics loss
            lambda_reg: Weight for regularization (smoothness)
        
        Returns:
            H: Feature matrix (M, n_neurons)
            b: Target vector (M,) complex
        """
        feat = self.features_mu
        N_colloc = X_colloc.shape[0]
        
        # ========== Physics Rows ==========
        # From PDE: Î¼Â·âˆ‡Â²u + ÏÏ‰Â²u = 0 (neglecting âˆ‡Î¼Â·âˆ‡u term initially)
        # Rearranged: Î¼Â·âˆ‡Â²u = -ÏÏ‰Â²u
        # 
        # In least squares form: [âˆ‡Â²u Â· Î¦(x)] @ weights_Î¼ = -ÏÏ‰Â²u
        # where Î¦(x) are the features for Î¼ network
        
        Phi_colloc = feat.phi(X_colloc)  # (N_colloc, n_neurons)
        
        # Weight each row by âˆ‡Â²u (element-wise multiplication)
        # H_physics[i,:] = âˆ‡Â²u[i] * Î¦[i,:] 
        H_physics = np.sqrt(lambda_physics) * (laplacian_u_current[:, None] * Phi_colloc)
        b_physics = np.sqrt(lambda_physics) * (-self.rho_omega_sq * u_current)
        
        # ========== Regularization Rows (Smoothness) ==========
        # Penalize ||âˆ‡Î¼||Â² by adding rows for âˆ‚Î¼/âˆ‚x, âˆ‚Î¼/âˆ‚y, âˆ‚Î¼/âˆ‚z â‰ˆ 0
        Phi_x = feat.phi_x(X_colloc)
        Phi_y = feat.phi_y(X_colloc)
        Phi_z = feat.phi_z(X_colloc)
        
        H_reg_x = np.sqrt(lambda_reg) * Phi_x
        H_reg_y = np.sqrt(lambda_reg) * Phi_y
        H_reg_z = np.sqrt(lambda_reg) * Phi_z
        
        b_reg = np.zeros(3 * N_colloc, dtype=complex)
        
        # ========== Stack ==========
        H = np.vstack([H_physics, H_reg_x, H_reg_y, H_reg_z])
        b = np.concatenate([b_physics, b_reg])
        
        return H, b
    
    def train(
        self,
        X_data: np.ndarray,
        u_measured: np.ndarray,
        X_colloc: np.ndarray,
        max_iterations: int = 50,
        lambda_data: float = 1.0,
        lambda_physics: float = 0.5,
        lambda_reg_schedule: list = [1.0, 0.1, 0.01],
        ridge: float = 1e-8,
        verbose: bool = True
    ):
        """
        Train iterative PIELM-MRE with curriculum learning.
        
        Args:
            X_data: Measured data points (N_data, 3)
            u_measured: Measured displacement (N_data,) complex
            X_colloc: Collocation points (N_colloc, 3)
            max_iterations: Maximum number of iterations
            lambda_data: Weight for data matching loss
            lambda_physics: Weight for physics loss
            lambda_reg_schedule: Regularization schedule [stage1, stage2, stage3]
            ridge: Ridge regularization for weight solving
            verbose: Print progress
        """
        print(f"\n{'='*60}")
        print("Starting Iterative PIELM-MRE Training")
        print(f"{'='*60}")
        print(f"ğŸ“Œ Training Configuration:")
        print(f"   Data points: {X_data.shape[0]:,}")
        print(f"   Collocation points: {X_colloc.shape[0]:,}")
        print(f"   Max iterations: {max_iterations}")
        print(f"   Î»_data: {lambda_data}")
        print(f"   Î»_physics: {lambda_physics}")
        print(f"   Î»_reg schedule: {lambda_reg_schedule}")
        print(f"   Ridge: {ridge}")
        print(f"{'='*60}\n")
        
        # Curriculum learning stages
        stage1_end = max_iterations // 3
        stage2_end = 2 * max_iterations // 3
        
        # Initialize Î¼ with homogeneous guess (5 kPa mean of 3-10 kPa range)
        # This helps avoid zero solutions
        mu_init_value = 5000.0 + 1j * self.omega * 1.0  # 5 kPa real + viscosity term
        mu_current = np.full(X_colloc.shape[0], mu_init_value, dtype=complex)
        
        # Pre-train Î¼ network with homogeneous initialization
        Phi_mu_init = self.features_mu.phi(X_colloc)
        target_init = np.full(X_colloc.shape[0], mu_init_value, dtype=complex)
        self.mu_network.solve_weights(Phi_mu_init, target_init, ridge=ridge)
        
        print(f"ğŸ”§ Initialized Î¼ network with homogeneous stiffness: {mu_init_value.real:.0f} Pa\n")
        
        for iteration in range(max_iterations):
            # Determine curriculum stage
            if iteration < stage1_end:
                lambda_reg = lambda_reg_schedule[0]
                stage = 1
            elif iteration < stage2_end:
                lambda_reg = lambda_reg_schedule[1]
                stage = 2
            else:
                lambda_reg = lambda_reg_schedule[2]
                stage = 3
            
            # ========== Step 1: Fix Î¼, solve for u ==========
            H_u, b_u = self.assemble_displacement_system(
                X_data, u_measured, X_colloc, mu_current,
                lambda_data, lambda_physics
            )
            
            self.u_network.solve_weights(H_u, b_u, ridge)
            
            # Predict current u at collocation points
            u_current = self.u_network.predict(X_colloc)
            
            # Compute derivatives of u from the trained network
            # Since u = Î£ wáµ¢Â·Ï†áµ¢(x), we have:
            #   âˆ‚u/âˆ‚x = Î£ wáµ¢Â·âˆ‚Ï†áµ¢/âˆ‚x
            #   âˆ‡Â²u = Î£ wáµ¢Â·âˆ‡Â²Ï†áµ¢
            
            feat_u = self.features_u
            
            # Gradient components (N_colloc, 3)
            grad_u_x = (feat_u.phi_x(X_colloc) @ self.u_network.weights_real) + \
                       1j * (feat_u.phi_x(X_colloc) @ self.u_network.weights_imag)
            grad_u_y = (feat_u.phi_y(X_colloc) @ self.u_network.weights_real) + \
                       1j * (feat_u.phi_y(X_colloc) @ self.u_network.weights_imag)
            grad_u_z = (feat_u.phi_z(X_colloc) @ self.u_network.weights_real) + \
                       1j * (feat_u.phi_z(X_colloc) @ self.u_network.weights_imag)
            
            grad_u_current = np.column_stack([grad_u_x, grad_u_y, grad_u_z])
            
            # Laplacian (N_colloc,)
            laplacian_u_current = (feat_u.laplacian(X_colloc) @ self.u_network.weights_real) + \
                                  1j * (feat_u.laplacian(X_colloc) @ self.u_network.weights_imag)
            
            # Debug: Check if Laplacian is non-zero
            if iteration == 0 or iteration % 10 == 0:
                lap_mag = np.abs(laplacian_u_current)
                print(f"   [Debug] âˆ‡Â²u range: [{lap_mag.min():.2e}, {lap_mag.max():.2e}], mean: {lap_mag.mean():.2e}")
            
            # ========== Step 2: Fix u, solve for Î¼ ==========
            H_mu, b_mu = self.assemble_modulus_system(
                X_colloc, u_current, grad_u_current, laplacian_u_current,
                lambda_physics, lambda_reg
            )
            
            self.mu_network.solve_weights(H_mu, b_mu, ridge)
            
            # Update Î¼ estimate
            mu_current = self.mu_network.predict(X_colloc)
            
            # ========== Compute Losses ==========
            u_pred_data = self.u_network.predict(X_data)
            loss_data = np.mean(np.abs(u_pred_data - u_measured)**2)
            
            # Physics residual at collocation points
            # PDE: Î¼âˆ‡Â²u + ÏÏ‰Â²u â‰ˆ 0
            pde_residual = mu_current * laplacian_u_current + self.rho_omega_sq * u_current
            loss_physics = np.mean(np.abs(pde_residual)**2)
            
            loss_total = lambda_data * loss_data + lambda_physics * loss_physics
            
            # Log history
            self.history['iteration'].append(iteration)
            self.history['loss_total'].append(loss_total)
            self.history['loss_data'].append(loss_data)
            self.history['loss_physics_u'].append(0.0)  # Placeholder
            self.history['loss_physics_mu'].append(loss_physics)
            self.history['loss_reg'].append(0.0)  # Placeholder
            
            if verbose and (iteration % 5 == 0 or iteration == max_iterations - 1):
                print(f"Iter {iteration:3d} | Stage {stage} | "
                      f"L_total: {loss_total:.2e} | L_data: {loss_data:.2e} | "
                      f"L_physics: {loss_physics:.2e} | Î»_reg: {lambda_reg:.2e}")
        
        print(f"\n{'='*60}")
        print("âœ… Training Complete!")
        print(f"{'='*60}\n")


# Test function
if __name__ == "__main__":
    print("Testing PIELM-MRE Implementation...\n")
    
    # Create dummy data
    np.random.seed(42)
    N_data = 1000
    N_colloc = 500
    
    X_data = np.random.rand(N_data, 3) * 0.1  # 0.1m cube
    u_measured = np.random.randn(N_data) + 1j * np.random.randn(N_data)
    X_colloc = np.random.rand(N_colloc, 3) * 0.1
    
    # Initialize solver
    solver = IterativePIELMMRE(n_neurons=200, frequency=60.0)
    
    # Train (just structure test)
    # solver.train(X_data, u_measured, X_colloc, max_iterations=10, verbose=True)
    
    print("\nâœ… PIELM-MRE structure validated!")
    print("ğŸ“Œ Next: Integrate with real BIOQIC data from Phase 1")
